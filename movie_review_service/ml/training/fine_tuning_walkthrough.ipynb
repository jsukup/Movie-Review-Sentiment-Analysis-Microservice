{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning DistilBERT for Sentiment Analysis\n",
        "\n",
        "This notebook walks through the process of fine-tuning a DistilBERT model for sentiment analysis using the IMDB movie reviews dataset.\n",
        "\n",
        "## Why DistilBERT?\n",
        "\n",
        "DistilBERT is a \"distilled\" version of BERT (Bidirectional Encoder Representations from Transformers) meaning that it retains 97% of its language understanding capabilities while being 40% smaller and 60% faster. This makes it ideal for:\n",
        "- Production deployments where resource efficiency is crucial\n",
        "- Quick experimentation and fine-tuning\n",
        "- Applications requiring real-time inference\n",
        "\n",
        "## What this Notebook Covers\n",
        "\n",
        "1. Data preparation and preprocessing\n",
        "2. Model configuration and optimization\n",
        "3. Training with performance monitoring\n",
        "4. Evaluation and metrics visualization\n",
        "5. Model saving and deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment Setup\n",
        "\n",
        "We'll start by setting up our environment:\n",
        "- Importing the necessary libraries\n",
        "- Setting random seeds for reproducibility\n",
        "- Optimizing memory usage for GPU training (CUDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import gc\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(2025)\n",
        "np.random.seed(2025)\n",
        "\n",
        "# Setup device and optimize memory usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.set_per_process_memory_fraction(0.8)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "We'll implement our dataset handling with the following features:\n",
        "- Efficient data loading using Hugging Face `datasets`\n",
        "- Balanced class distribution using stratified sampling (the IMDb dataset is already balanced but we'll use a subset for demonstration purposes)\n",
        "- Memory-efficient PyTorch Dataset implementation\n",
        "- Label distribution visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_label_distribution(data, split_name):\n",
        "    \"\"\"Print and visualize the distribution of labels in a dataset\"\"\"\n",
        "    labels = data[\"label\"]\n",
        "    label_counts = Counter(labels)\n",
        "    total = len(labels)\n",
        "    \n",
        "    print(f\"\\n{split_name} Label Distribution:\")\n",
        "    counts = []\n",
        "    names = []\n",
        "    for label, count in sorted(label_counts.items()):\n",
        "        label_name = \"Positive\" if label == 1 else \"Negative\"\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"{label_name}: {count} ({percentage:.2f}%)\")\n",
        "        counts.append(count)\n",
        "        names.append(label_name)\n",
        "    \n",
        "    # Visualize distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(names, counts)\n",
        "    plt.title(f\"{split_name} Label Distribution\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.encodings = tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data(num_samples=None, test_size=0.2, random_state=2025):\n",
        "    \"\"\"Load and prepare IMDB dataset with visualization\"\"\"\n",
        "    print(f\"Loading IMDB dataset{f' (using {num_samples} samples)' if num_samples else ''}\")\n",
        "    \n",
        "    dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "    df = pd.DataFrame({\"text\": dataset[\"text\"], \"label\": dataset[\"label\"]})\n",
        "    \n",
        "    if num_samples:\n",
        "        df = df.groupby(\"label\", group_keys=False)\\\n",
        "               .apply(lambda x: x.sample(n=num_samples // 2, random_state=random_state))\\\n",
        "               .reset_index(drop=True)\n",
        "    \n",
        "    train_data, val_data = train_test_split(\n",
        "        df, test_size=test_size, random_state=random_state, stratify=df[\"label\"]\n",
        "    )\n",
        "    \n",
        "    # Convert to dictionary format\n",
        "    train_dict = {\n",
        "        \"text\": train_data[\"text\"].tolist(),\n",
        "        \"label\": train_data[\"label\"].tolist(),\n",
        "    }\n",
        "    val_dict = {\n",
        "        \"text\": val_data[\"text\"].tolist(),\n",
        "        \"label\": val_data[\"label\"].tolist(),\n",
        "    }\n",
        "    \n",
        "    # Print and plot distributions\n",
        "    print_label_distribution(train_dict, \"Training\")\n",
        "    print_label_distribution(val_dict, \"Validation\")\n",
        "    \n",
        "    print(f\"\\nTrain size: {len(train_dict['text'])}, Validation size: {len(val_dict['text'])}\")\n",
        "    return train_dict, val_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's prepare our data with a smaller sample size for quick experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data with 10000 samples for testing (use 25000 for full training)\n",
        "train_data, val_data = prepare_data(num_samples=10000)\n",
        "\n",
        "# Display a sample review\n",
        "print(\"\\nSample review:\")\n",
        "print(f\"Text: {train_data['text'][0][:200]}...\")\n",
        "print(f\"Label: {'Positive' if train_data['label'][0] == 1 else 'Negative'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Configuration\n",
        "\n",
        "Now we'll set up our DistilBERT model with optimizations for fine-tuning:\n",
        "- Frozen base layers to prevent catastrophic forgetting and reduce training time\n",
        "- Memory optimizations for efficient training\n",
        "- Gradient checkpointing for larger batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_model(model_name=\"distilbert-base-uncased\", freeze_base_model=True):\n",
        "    \"\"\"Initialize and configure the model for fine-tuning\"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    \n",
        "    if freeze_base_model:\n",
        "        # Freeze base model layers\n",
        "        for param in model.distilbert.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # Keep classification layers trainable\n",
        "        for param in model.pre_classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in model.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    # Print parameter statistics\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
        "    \n",
        "    # Enable memory optimization\n",
        "    model.config.use_cache = False\n",
        "    model.gradient_checkpointing_enable()\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training and Evaluation Functions\n",
        "\n",
        "We'll implement comprehensive evaluation metrics and a robust training loop. These functions are taken directly from our training scripts with minor adaptations for interactive notebook use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model with multiple metrics\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing various metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            total_loss += outputs.loss.item()\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    report = classification_report(\n",
        "        all_labels,\n",
        "        all_predictions,\n",
        "        target_names=[\"Negative\", \"Positive\"],\n",
        "        output_dict=True,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"precision\": report[\"macro avg\"][\"precision\"],\n",
        "        \"recall\": report[\"macro avg\"][\"recall\"],\n",
        "        \"f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"class_metrics\": {\n",
        "            \"negative\": {\n",
        "                \"precision\": report[\"Negative\"][\"precision\"],\n",
        "                \"recall\": report[\"Negative\"][\"recall\"],\n",
        "                \"f1\": report[\"Negative\"][\"f1-score\"],\n",
        "            },\n",
        "            \"positive\": {\n",
        "                \"precision\": report[\"Positive\"][\"precision\"],\n",
        "                \"recall\": report[\"Positive\"][\"recall\"],\n",
        "                \"f1\": report[\"Positive\"][\"f1-score\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "def print_metrics(metrics, split=\"Validation\"):\n",
        "    \"\"\"Pretty print the metrics\"\"\"\n",
        "    print(f\"\\n{split} Metrics:\")\n",
        "    print(f\"Loss: {metrics['loss']:.4f}\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision (macro): {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall (macro): {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score (macro): {metrics['f1']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    for class_name, class_metrics in metrics[\"class_metrics\"].items():\n",
        "        print(f\"\\n{class_name.capitalize()}:\")\n",
        "        print(f\"  Precision: {class_metrics['precision']:.4f}\")\n",
        "        print(f\"  Recall: {class_metrics['recall']:.4f}\")\n",
        "        print(f\"  F1 Score: {class_metrics['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop\n",
        "\n",
        "Now we'll implement our training loop with hyperparameter optimizations for training speed and memory efficiency (my laptop GPU is not very powerful):\n",
        "- Learning rate scheduling\n",
        "- Gradient accumulation\n",
        "- Memory optimization\n",
        "- Progress tracking\n",
        "\n",
        "Note: This implementation is adapted from the training script (train.py) with modifications for interactive use in the notebook. The core functionality remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    output_dir=\"/root/freelance-labs/movie_review_service/app/ml/models/best_model\"\n",
        "):\n",
        "    \"\"\"Train the model with comprehensive logging\"\"\"\n",
        "    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
        "    total_steps = (len(train_loader) // 4) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
        "    )\n",
        "    \n",
        "    # Create output directory\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Training loop\n",
        "    best_accuracy = 0\n",
        "    start_time = time.time()\n",
        "    \n",
        "    metrics_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_accuracy': [], 'val_accuracy': [],\n",
        "        'train_f1': [], 'val_f1': []\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        epoch_start = time.time()\n",
        "        \n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss / 4  # gradient accumulation\n",
        "            loss.backward()\n",
        "            \n",
        "            if (i + 1) % 4 == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            total_loss += loss.item() * 4\n",
        "            \n",
        "            # Clear cache periodically\n",
        "            if i % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # Evaluate\n",
        "        train_metrics = evaluate_model(model, train_loader, device)\n",
        "        val_metrics = evaluate_model(model, val_loader, device)\n",
        "        \n",
        "        # Store metrics\n",
        "        metrics_history['train_loss'].append(train_metrics['loss'])\n",
        "        metrics_history['val_loss'].append(val_metrics['loss'])\n",
        "        metrics_history['train_accuracy'].append(train_metrics['accuracy'])\n",
        "        metrics_history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "        metrics_history['train_f1'].append(train_metrics['f1'])\n",
        "        metrics_history['val_f1'].append(val_metrics['f1'])\n",
        "        \n",
        "        # Print metrics\n",
        "        print_metrics(train_metrics, \"Training\")\n",
        "        print_metrics(val_metrics, \"Validation\")\n",
        "        \n",
        "        # Save best model\n",
        "        if val_metrics[\"accuracy\"] > best_accuracy:\n",
        "            best_accuracy = val_metrics[\"accuracy\"]\n",
        "            print(f\"\\nNew best accuracy: {best_accuracy:.4f}! Saving model...\")\n",
        "            model.save_pretrained(output_dir)\n",
        "            tokenizer.save_pretrained(output_dir)\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"\\nEpoch time: {epoch_time:.2f} seconds\")\n",
        "        \n",
        "        # Clear cache between epochs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
        "    print(f\"Best accuracy: {best_accuracy:.4f}\")\n",
        "    \n",
        "    return metrics_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train the Model\n",
        "\n",
        "Let's train our model and visualize the results. Note that we're using a smaller dataset for demonstration purposes. For production use, you should use the full dataset (25,000 samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model and tokenizer\n",
        "model, tokenizer = initialize_model()\n",
        "model = model.to(device)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = IMDBDataset(\n",
        "    texts=train_data[\"text\"],\n",
        "    labels=train_data[\"label\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "val_dataset = IMDBDataset(\n",
        "    texts=val_data[\"text\"],\n",
        "    labels=val_data[\"label\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "metrics_history = train_model(model, train_loader, val_loader, num_epochs=3)\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(metrics_history['train_loss'], label='Training')\n",
        "plt.plot(metrics_history['val_loss'], label='Validation')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(metrics_history['train_accuracy'], label='Training')\n",
        "plt.plot(metrics_history['val_accuracy'], label='Validation')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(metrics_history['train_f1'], label='Training')\n",
        "plt.plot(metrics_history['val_f1'], label='Validation')\n",
        "plt.title('F1 Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Model\n",
        "\n",
        "So, we'd probably have better results if we used a larger dataset and more epochs, but this is just a demo. Let's test our model on some sample reviews. \n",
        "\n",
        "## Note on Three-Class Classification\n",
        "\n",
        "While this model is trained on binary labels (positive/negative), our inference pipeline \n",
        "supports three-class classification (positive/negative/neutral) by using a `confidence threshold`:\n",
        "\n",
        "- If prediction confidence < 0.55: Classify as \"neutral\"\n",
        "- Otherwise: Use model's binary prediction (positive/negative)\n",
        "\n",
        "This approach allows us to identify reviews with ambiguous sentiment without requiring three-class training data. It's set to 0.55 here since it seems that none of the reviews tested by the fine-tuned model ever had a confidence score below 0.50. Confidence scores would presumably be higher for a larger dataset and more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, confidence_threshold=0.55):\n",
        "    \"\"\"Make a prediction with confidence score\"\"\"\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "        confidence, prediction = torch.max(probabilities, dim=1)\n",
        "        \n",
        "    confidence = confidence.item()\n",
        "    if confidence < confidence_threshold:\n",
        "        sentiment = \"Neutral\"\n",
        "    else:\n",
        "        sentiment = \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
        "    \n",
        "    return sentiment, confidence\n",
        "\n",
        "# Test some sample reviews\n",
        "sample_reviews = [\n",
        "    \"Best movie I've seen in a long time!\",\n",
        "    \"I was really disappointed with this film. The story was confusing and the characters were poorly developed.\",\n",
        "    \"While not perfect, the movie had its moments. Some scenes were great while others fell flat.\"\n",
        "]\n",
        "\n",
        "print(\"Sample Predictions:\\n\")\n",
        "for review in sample_reviews:\n",
        "    sentiment, confidence = predict_sentiment(review, model, tokenizer)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Differences from Production Script\n",
        "\n",
        "1. The notebook version omits Hugging Face Hub model pushing (which requires authentication)\n",
        "2. We've added interactive visualizations not present in the production script\n",
        "3. The training loop includes additional metrics tracking for visualization\n",
        "4. Memory management is adapted for interactive use\n",
        "\n",
        "The core model architecture, training process, and inference logic remain identical to the production implementation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
